# pinyin-input

Pinyin input method: HomeWork1 of Artificial Intelligence Lesson


### 一、实验环境介绍

#### 1.1 电脑软硬件配置

- **软件平台：**

    操作系统：Windows 10家庭中文版，64位操作系统

    开发软件：PyCharm professional 2020.1	

- **硬件配置：**

    CPU：Intel i7-10710U

    内存：16GB

- **编译环境：**

    编程语言：Python

    依赖库：tqdm、numpy、pypinyin、jieba、re、argparse

#### 1.2 开源代码

- github链接（代码，不包含统计好的基于字的计数）：https://github.com/jiahy0825/pinyin-input
- 全部预训练字计数：以json文件保存，在作业附件的压缩包中

### 二、算法思路与实现

#### 2.1 语料库的预训练（线下已完成本步骤，此步可省略）

```python
if __name__ == "__main__":
    prep = Preprocess()
    # 读取拼音到汉字映射
    prep.read_pinyin2word(args.pinyin_word_file, args.filter_path)
	# 读取训练数据，统计基于字模型中，每个字以及对应二元模型、三元模型的出现次数
    prep.read_train_file(args.train_path + "2016-02.txt")
    for month in tqdm(range(4, 12)):
        prep.read_train_file(args.train_path + "2016-" + str(month).zfill(2) + ".txt")
	# 将统计结果保存到trigram_pinyin_wordcnt.json文件中
    prep.save_word_cnt()
```

首先运行`python preprocess.py`命令，实现语料库的预训练，由于线下已完成三元模型预训练步骤，并将结果保存到对应的文件中，因此此步可省略。如果想重新训练，可以执行下列命令

```python
# 需要首先在 param.py 中修改拼音汉字表.txt、一二级汉字表.txt、sina_news_gbk的位置，或者在执行过程中显式声明
# 二元模型预训练
python preprocess.py --type=bigram
# 三元模型预训练
python preprocess.py --type=trigram
```

预训练思想：

1. 首先读取`一二级汉字表.txt`文件，统计出合法的7千多个汉字。
2. 读取`拼音汉字表.txt`，实现每个拼音和汉字的对应关系。
3. 读取语料库`sina_news_gbk`，统计二元模型、三元模型所需的出现次数统计：
    - 只统计每个网页中标题和正文的语句，统计句子数为49491543个
    - 对每句话进行划分，把一串连续的中文字当作一个中文句子

#### 2.2 基于字的二元模型

运行方式（例）：

```python
# 输入文件为 input.txt，输出文件为 output.txt，使用三元模型
python model.py --input_file=input.txt --output_file=output.txt --type=trigram
```

##### 2.2.1 假设语言满足2阶马尔可夫性，且字之间独立

2阶马尔可夫性：假设句子中当前字的出现只与前一个字有关，即$h_i=w_{i-1}$
$$
P(w_i|h_i)=P(w_i|w_{i-1})
$$
例如：
$$
清=argmax(qing|B) \\
华=argmax(hua|清) \\
大=argmax(da|华) \\
学=argmax(xue|大)
$$
在实现细节方面，为每个句子的开头定义了一个开始字"B"，此时根据当前拼音，不断预测下一个字。

但是基于字之间独立的假设，意味着每个字都只与前一个字有关，这种假设是最简单的一种假设。

##### 2.2.2 假设语言满足2阶马尔可夫性，HMM模型

要从所有可行句子中，选取出现概率最大的那句话。

例如：
$$
P(清华大学)=P(清|B)*P(华|清)*P(大|华)*P(学|大)
$$
句子`清华大学`是拼音串`qing hua da xue`可以输出的概率最大的句子，因此输出这个句子。

在具体实现过程中，如果计算每一个句子可能出现的概率，会为CPU和内存带来极大的压力，因此使用维特比算法进行优化，其本质是一种动态规划算法。

#### 2.3 基于字的三元模型

##### 2.3.1 假设语言满足3阶马尔可夫性，且字之间独立

3阶马尔可夫性：假设句子中当前字的出现只与前两个字有关，即$h_i=w_{i-2}, w_{i-1}$
$$
P(w_i|h_i)=P(w_i|w_{i-2}, w_{i-1})
$$
例如：
$$
清=argmax(qing|B1,B2) \\华=argmax(hua|B2,清) \\大=argmax(da|清华) \\学=argmax(xue|华大)
$$
在实现细节方面，为每个句子的开头定义了两个开始字"B1", "B2"，此时根据当前拼音，不断预测下一个字。

但是基于字之间独立的假设，意味着每个字都只与前两个字有关，这种假设是最简单的一种假设。

##### 2.3.2 假设语言满足3阶马尔可夫性，HMM模型

要从所有可行句子中，选取出现概率最大的那句话。

例如：
$$
P(清华大学)=P(清|B1, B2)*P(华|B2, 清)*P(大|清, 华)*P(学|华, 大)
$$
句子`清华大学`是拼音串`qing hua da xue`可以输出的概率最大的句子，因此输出这个句子。

在具体实现过程中，如果计算每一个句子可能出现的概率，会为CPU和内存带来极大的压力，因此使用维特比算法进行优化。

#### 2.4 额外的实现细节

##### 2.4.1 tqdm

在进行语料库的训练过程中，使用tdqm库来进行管理，从而得知训练进度。目前的训练过程通常需要3-4小时才可以完成，因此在模型应用过程中，把统计结果以json文件进行保存。

##### 2.4.2 pypinyin

读取语料库时，由于语料库中只有中文字，而没有对应的拼音，这导致对于多音字的效果不好，因此使用`pypinyin`库来对每句话进行拼音标注，并将拼音与字结合统计对应的计数。

##### 2.4.3 argparse

使用`argparse`库进行参数管理，将默认参数保存至`param.py`文件中，可以在执行代码时，选择不同的运行方式和训练语料、语言模型等等。

### 三、实现效果和性能分析

使用网络学堂上提供的`输入法测试集.txt`文件进行测试，所得到的结果如下图所示

#### 3.1 不使用pypinyin的结果

| 准确率   | 字间独立模型   | HMM模型       |
| -------- | -------------- | ------------- |
| 二元模型 | 6.24%(38/609)  | 6.40%(39/609) |
| 三元模型 | 13.46%(82/609) | 0.0%(0/609)   |

可以看到准确率最高的模型是字间独立的三元模型，而基于HMM模型的准确率低的原因，可能是因为语料库的数量太小，导致不能进行合理的预测。

考虑拼音输出情况，可以看到多音字的情况，较大的影响了模型效果。

例如：

qing hua da xue ji suan ji xi：清华大学期算机系

由于`期`是一个多音字，当其读音是`ji`时出现的次数其实很少，但是如果不考虑拼音的话，会导致`xue ji suan ji`的输出为`学期算机`，因此下一步考虑为每一个字分配拼音。

#### 3.2 使用pypinyin优化

| 准确率   | 字间独立模型   | HMM模型       |
| -------- | -------------- | ------------- |
| 二元模型 | 7.22%(44/609)  | 8.05%(49/609) |
| 三元模型 | 14.77%(90/609) | 1.47%(9/609)  |

加入每个字的读音后，可以看到每个模型都有对应的提升，其中基于字间独立假设的三元模型效果最好，因此模型运行中的默认方式，采用了这个模型。

并且上一节中对应的问题，可以得到正确解：

qing hua da xue ji suan ji xi：清华大学计算机系

### 四、总结与展望

​		在完成拼音输入法作业的过程中，通过使用隐式马尔可夫模型和维特比算法，可以完成拼音与句子的转化，但是由于语料库不多，因此该模型不能得到较高的准确率。进一步假设字间独立的话，对应的三元模型可以得到更好地效果。此外，在项目的完成过程中，通过使用tqdm、argparse等库，进一步优化了与用户的交互过程。

​		优化本拼音输入法，可以有多种方式，例如引入更多的语料库（例如中文维基、百度文档）等，并且也可以尝试对二元模型和三元模型中词频的出现方式，选取不同的平滑方法。相信这些方法还可以进一步提升模型效果。